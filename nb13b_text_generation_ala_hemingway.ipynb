{"cells":[{"cell_type":"markdown","metadata":{},"source":["# CS 39AA - Notebook 13b: Text Generation with fine-tuned GPT-2\n","\n","Let's now see what kind of results we can get if we take the same model but fine tune on what some say is Hemingway's best novel, 'The Sun Also Rises'. "]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import torch\n","from torch.utils.data import Dataset, random_split\n","from transformers import GPT2Tokenizer, TrainingArguments, Trainer, GPT2LMHeadModel\n","from transformers import set_seed"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"data":{"text/plain":["Embedding(50259, 1024)"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["MODEL_NAME = 'gpt2-medium'\n","\n","tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME, bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>')\n","model = GPT2LMHeadModel.from_pretrained(MODEL_NAME)\n","model.resize_token_embeddings(len(tokenizer))"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":["0    Robert Cohn was once middleweight boxing champ...\n","1    Do not think that I am very much impressed by ...\n","2    He cared nothing for boxing, in fact he dislik...\n","3    There was a certain inner comfort in knowing h...\n","4                    He was Spider Kellyâ€™s star pupil.\n","Name: sentence, dtype: object"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["sentences = pd.read_csv('data/sunalsorises.csv')['sentence']\n","sentences.head()"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["max_length = max([len(tokenizer.encode(sentence)) for sentence in sentences])"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/plain":["224"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["max_length"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["class HemingwayDataset(Dataset):\n","    def __init__(self, txt_list, tokenizer, max_length):\n","        self.input_ids = []\n","        self.attn_masks = []\n","        self.labels = []\n","        for txt in txt_list:\n","            encodings_dict = tokenizer('<|startoftext|>' + txt + '<|endoftext|>', truncation=True,\n","                                       max_length=max_length, padding=\"max_length\")\n","            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n","            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n","\n","    def __len__(self): # overload the len() Python built-in function\n","        return len(self.input_ids)\n","\n","    def __getitem__(self, idx): # overload the [] operator\n","        return self.input_ids[idx], self.attn_masks[idx]\n","\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["dataset = HemingwayDataset(sentences, tokenizer, max_length=max_length)\n","train_size = int(0.9 * len(dataset))\n","train_dataset, val_dataset = random_split(dataset, [train_size, len(dataset) - train_size])"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/plain":["(tensor([50257,   447,   250,  5812,    11, 18726,    11,   314,  1842,   345,\n","           523,   881,    13, 50256, 50258, 50258, 50258, 50258, 50258, 50258,\n","         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n","         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n","         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n","         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n","         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n","         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n","         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n","         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n","         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n","         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n","         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n","         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n","         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n","         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n","         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n","         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n","         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n","         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n","         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n","         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n","         50258, 50258, 50258, 50258]),\n"," tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0]))"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["train_dataset[0]"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/plain":["584"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["import gc\n","gc.collect()"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["training_args = TrainingArguments(output_dir='/Users/steve/models/hemingway_generation', num_train_epochs=1, logging_steps=100, save_steps=5000,\n","                                  per_device_train_batch_size=1, per_device_eval_batch_size=1,\n","                                  warmup_steps=10, weight_decay=0.05, logging_dir='/Users/steve/models/hemingway_generation/logs', report_to = 'none')"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/steve/opt/anaconda3/envs/py311/lib/python3.11/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"da8b91e192f8484ca08ce010f7968e76","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/6143 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'loss': 0.7004, 'learning_rate': 4.9266264470895154e-05, 'epoch': 0.02}\n","{'loss': 0.2363, 'learning_rate': 4.845100277188978e-05, 'epoch': 0.03}\n","{'loss': 0.2571, 'learning_rate': 4.76357410728844e-05, 'epoch': 0.05}\n","{'loss': 0.207, 'learning_rate': 4.6820479373879014e-05, 'epoch': 0.07}\n","{'loss': 0.187, 'learning_rate': 4.6005217674873634e-05, 'epoch': 0.08}\n","{'loss': 0.2271, 'learning_rate': 4.5189955975868254e-05, 'epoch': 0.1}\n","{'loss': 0.2163, 'learning_rate': 4.4374694276862874e-05, 'epoch': 0.11}\n","{'loss': 0.2242, 'learning_rate': 4.3559432577857494e-05, 'epoch': 0.13}\n","{'loss': 0.202, 'learning_rate': 4.2744170878852114e-05, 'epoch': 0.15}\n","{'loss': 0.2067, 'learning_rate': 4.1928909179846734e-05, 'epoch': 0.16}\n","{'loss': 0.2326, 'learning_rate': 4.111364748084135e-05, 'epoch': 0.18}\n","{'loss': 0.22, 'learning_rate': 4.0298385781835974e-05, 'epoch': 0.2}\n","{'loss': 0.2208, 'learning_rate': 3.9483124082830593e-05, 'epoch': 0.21}\n","{'loss': 0.1875, 'learning_rate': 3.8667862383825207e-05, 'epoch': 0.23}\n","{'loss': 0.187, 'learning_rate': 3.7852600684819827e-05, 'epoch': 0.24}\n","{'loss': 0.2223, 'learning_rate': 3.7037338985814446e-05, 'epoch': 0.26}\n","{'loss': 0.2207, 'learning_rate': 3.6222077286809066e-05, 'epoch': 0.28}\n","{'loss': 0.1859, 'learning_rate': 3.5406815587803686e-05, 'epoch': 0.29}\n","{'loss': 0.1989, 'learning_rate': 3.4591553888798306e-05, 'epoch': 0.31}\n","{'loss': 0.1766, 'learning_rate': 3.3776292189792926e-05, 'epoch': 0.33}\n","{'loss': 0.1862, 'learning_rate': 3.296103049078754e-05, 'epoch': 0.34}\n","{'loss': 0.1838, 'learning_rate': 3.2145768791782166e-05, 'epoch': 0.36}\n","{'loss': 0.1982, 'learning_rate': 3.1330507092776786e-05, 'epoch': 0.37}\n","{'loss': 0.2321, 'learning_rate': 3.0515245393771403e-05, 'epoch': 0.39}\n","{'loss': 0.1941, 'learning_rate': 2.969998369476602e-05, 'epoch': 0.41}\n","{'loss': 0.1792, 'learning_rate': 2.888472199576064e-05, 'epoch': 0.42}\n","{'loss': 0.2054, 'learning_rate': 2.8069460296755262e-05, 'epoch': 0.44}\n","{'loss': 0.1993, 'learning_rate': 2.725419859774988e-05, 'epoch': 0.46}\n","{'loss': 0.1798, 'learning_rate': 2.64389368987445e-05, 'epoch': 0.47}\n","{'loss': 0.2238, 'learning_rate': 2.5623675199739115e-05, 'epoch': 0.49}\n","{'loss': 0.2103, 'learning_rate': 2.480841350073374e-05, 'epoch': 0.5}\n","{'loss': 0.2182, 'learning_rate': 2.3993151801728355e-05, 'epoch': 0.52}\n","{'loss': 0.1705, 'learning_rate': 2.3177890102722975e-05, 'epoch': 0.54}\n","{'loss': 0.2075, 'learning_rate': 2.2362628403717595e-05, 'epoch': 0.55}\n","{'loss': 0.2104, 'learning_rate': 2.154736670471221e-05, 'epoch': 0.57}\n","{'loss': 0.1926, 'learning_rate': 2.0732105005706835e-05, 'epoch': 0.59}\n","{'loss': 0.1899, 'learning_rate': 1.991684330670145e-05, 'epoch': 0.6}\n","{'loss': 0.1774, 'learning_rate': 1.910158160769607e-05, 'epoch': 0.62}\n","{'loss': 0.2204, 'learning_rate': 1.828631990869069e-05, 'epoch': 0.63}\n","{'loss': 0.157, 'learning_rate': 1.7471058209685308e-05, 'epoch': 0.65}\n","{'loss': 0.1669, 'learning_rate': 1.665579651067993e-05, 'epoch': 0.67}\n","{'loss': 0.1892, 'learning_rate': 1.5840534811674548e-05, 'epoch': 0.68}\n","{'loss': 0.1672, 'learning_rate': 1.5025273112669166e-05, 'epoch': 0.7}\n","{'loss': 0.1872, 'learning_rate': 1.4210011413663788e-05, 'epoch': 0.72}\n","{'loss': 0.1686, 'learning_rate': 1.3394749714658406e-05, 'epoch': 0.73}\n","{'loss': 0.1918, 'learning_rate': 1.2579488015653026e-05, 'epoch': 0.75}\n","{'loss': 0.218, 'learning_rate': 1.1764226316647644e-05, 'epoch': 0.77}\n","{'loss': 0.1899, 'learning_rate': 1.0948964617642264e-05, 'epoch': 0.78}\n","{'loss': 0.1958, 'learning_rate': 1.0133702918636882e-05, 'epoch': 0.8}\n","{'loss': 0.1869, 'learning_rate': 9.318441219631502e-06, 'epoch': 0.81}\n","{'loss': 0.1835, 'learning_rate': 8.503179520626122e-06, 'epoch': 0.83}\n","{'loss': 0.2012, 'learning_rate': 7.68791782162074e-06, 'epoch': 0.85}\n","{'loss': 0.1643, 'learning_rate': 6.87265612261536e-06, 'epoch': 0.86}\n","{'loss': 0.1527, 'learning_rate': 6.057394423609979e-06, 'epoch': 0.88}\n","{'loss': 0.1864, 'learning_rate': 5.2421327246045984e-06, 'epoch': 0.9}\n","{'loss': 0.1814, 'learning_rate': 4.4268710255992175e-06, 'epoch': 0.91}\n","{'loss': 0.1783, 'learning_rate': 3.6116093265938366e-06, 'epoch': 0.93}\n","{'loss': 0.2387, 'learning_rate': 2.796347627588456e-06, 'epoch': 0.94}\n","{'loss': 0.1903, 'learning_rate': 1.981085928583075e-06, 'epoch': 0.96}\n","{'loss': 0.2116, 'learning_rate': 1.1658242295776945e-06, 'epoch': 0.98}\n","{'loss': 0.2047, 'learning_rate': 3.505625305723137e-07, 'epoch': 0.99}\n","{'train_runtime': 9036.0197, 'train_samples_per_second': 0.68, 'train_steps_per_second': 0.68, 'train_loss': 0.20672675171097152, 'epoch': 1.0}\n"]},{"data":{"text/plain":["TrainOutput(global_step=6143, training_loss=0.20672675171097152, metrics={'train_runtime': 9036.0197, 'train_samples_per_second': 0.68, 'train_steps_per_second': 0.68, 'train_loss': 0.20672675171097152, 'epoch': 1.0})"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["Trainer(model=model,  args=training_args, train_dataset=train_dataset, \n","        eval_dataset=val_dataset, data_collator=lambda data: {'input_ids': torch.stack([f[0] for f in data]),\n","                                                              'attention_mask': torch.stack([f[1] for f in data]),\n","                                                              'labels': torch.stack([f[0] for f in data])}).train()"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["model.save_pretrained(\"/Users/steve/models/hemingway_generation\")"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"text/plain":["('/Users/steve/models/hemingway_generation/tokenizer_config.json',\n"," '/Users/steve/models/hemingway_generation/special_tokens_map.json',\n"," '/Users/steve/models/hemingway_generation/vocab.json',\n"," '/Users/steve/models/hemingway_generation/merges.txt',\n"," '/Users/steve/models/hemingway_generation/added_tokens.json')"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.save_pretrained(\"/Users/steve/models/hemingway_generation\")"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["tokenizer = GPT2Tokenizer.from_pretrained(\"/Users/steve/models/hemingway_generation\")\n","model = GPT2LMHeadModel.from_pretrained(\"/Users/steve/models/hemingway_generation\")"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["set_seed(41)\n","generated = tokenizer(\"<|startoftext|> The old bullfighter fell and\", return_tensors=\"pt\").input_ids\n","sample_outputs = model.generate(generated, do_sample=True, top_k=1000, max_length=20, temperature=0.5, num_return_sequences=20, pad_token_id=tokenizer.eos_token_id)"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["ret_seq0: <|startoftext|>  The old bullfighter fell and the old man was dead.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|> \n","    (len(generated) = 11) \n","\n","ret_seq1: <|startoftext|>  The old bullfighter fell and the old bullfighter fell.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|> \n","    (len(generated) = 10) \n","\n","ret_seq2: <|startoftext|>  The old bullfighter fell and it was all over.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|> \n","    (len(generated) = 10) \n","\n","ret_seq3: <|startoftext|>  The old bullfighter fell and hit the ground.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|> \n","    (len(generated) = 9) \n","\n","ret_seq4: <|startoftext|>  The old bullfighter fell and hit the bull down, his sword in his hand.<|endoftext|><|endoftext|> \n","    (len(generated) = 15) \n","\n","ret_seq5: <|startoftext|>  The old bullfighter fell and fell and fell, and the big man got up and started to \n","    (len(generated) = 18) \n","\n","ret_seq6: <|startoftext|>  The old bullfighter fell and the old man came out of the crowd and was knocked down. \n","    (len(generated) = 18) \n","\n","ret_seq7: <|startoftext|>  The old bullfighter fell and the old man who had been in the fight fell, too, \n","    (len(generated) = 17) \n","\n","ret_seq8: <|startoftext|>  The old bullfighter fell and was dead.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|> \n","    (len(generated) = 8) \n","\n","ret_seq9: <|startoftext|>  The old bullfighter fell and fell, and the old bullfighter went up and fell.<|endoftext|> \n","    (len(generated) = 15) \n","\n","ret_seq10: <|startoftext|>  The old bullfighter fell and fell again.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|> \n","    (len(generated) = 8) \n","\n","ret_seq11: <|startoftext|>  The old bullfighter fell and the man who had been his lead fighter came out of the crowd \n","    (len(generated) = 19) \n","\n","ret_seq12: <|startoftext|>  The old bullfighter fell and the old man picked him up and put him on his back. \n","    (len(generated) = 18) \n","\n","ret_seq13: <|startoftext|>  The old bullfighter fell and we all went down and into the ring.<|endoftext|><|endoftext|><|endoftext|><|endoftext|> \n","    (len(generated) = 14) \n","\n","ret_seq14: <|startoftext|>  The old bullfighter fell and the bullfighter fell, and the bullfighter fell.<|endoftext|><|endoftext|> \n","    (len(generated) = 13) \n","\n","ret_seq15: <|startoftext|>  The old bullfighter fell and they were all dead.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|> \n","    (len(generated) = 10) \n","\n","ret_seq16: <|startoftext|>  The old bullfighter fell and we all went over to the bull-ring and charged.<|endoftext|> \n","    (len(generated) = 15) \n","\n","ret_seq17: <|startoftext|>  The old bullfighter fell and he was killed.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|> \n","    (len(generated) = 9) \n","\n","ret_seq18: <|startoftext|>  The old bullfighter fell and the old bullfighter went down in the ring.<|endoftext|><|endoftext|><|endoftext|> \n","    (len(generated) = 14) \n","\n","ret_seq19: <|startoftext|>  The old bullfighter fell and the crowd took him in and held him, and he went over \n","    (len(generated) = 18) \n","\n"]}],"source":["for i in range(len(sample_outputs)):\n","    generated = tokenizer.decode(sample_outputs[i])\n","    len_gen = len(sample_outputs[i])\n","    generated = generated.replace('\\n', ' ') # remove new line characters from generated text\n","    print(f\"ret_seq{i}: {generated} \\n    (len(generated) = {len(generated.split())}) \\n\")\n"]},{"cell_type":"markdown","metadata":{},"source":["Some references for this notebook are: \n","* https://www.kaggle.com/code/nulldata/fine-tuning-gpt-2-to-generate-netlfix-descriptions/notebook\n","* https://medium.com/geekculture/fine-tune-eleutherai-gpt-neo-to-generate-netflix-movie-descriptions-in-only-47-lines-of-code-40c9b4c32475\n","\n"]}],"metadata":{"kernelspec":{"display_name":"torch13","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"67f030e82dc83e46e375c9862143c2f050702bf26aeee9494179d0591d712143"}}},"nbformat":4,"nbformat_minor":2}
