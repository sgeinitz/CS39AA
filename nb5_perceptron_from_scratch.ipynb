{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 39AA - Notebook 5: A Perceptron from Scratch\n",
    "\n",
    "In this notebook we'll look get a good look at a simple neural network, a single layer perceptron, and at how this is directly related to logistic regression. We'll also use gradient descent to find the optimal parameter values for our model, based on the training data that we have. \n",
    "\n",
    "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/sgeinitz/CS39AA/blob/main/nb5_perceptron_from_scratch.ipynb)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sgeinitz/CS39AA/blob/main/nb5_perceptron_from_scratch.ipynb)\n",
    "\n",
    "\n",
    "## I. Logistic Regression\n",
    "\n",
    "### 1: Generate data\n",
    "We'll first import the necessary Python modules and then generate synthetic data with appropriate size/dimensions. \n",
    "\n",
    "Let's now plot the generated data to see x and y. Note that the y outcomes/labels belong to one of two classes. The positive cases are shown in blue while negative cases are in red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "N = 100 # total number of observations\n",
    "D_in = 1 # input dimension (i.e. dimension of a single observation's x vector)\n",
    "D_out = 1 # output dimension (i.e. y), so just 1 for this example\n",
    "random.seed(1)\n",
    "np.random.RandomState(1)\n",
    "\n",
    "# Create random input data and derive the 'true' labels/output\n",
    "x = np.random.randn(N, D_in) + 1 \n",
    "def true_y(x_in, n_obs):\n",
    "    def addNoise(x):\n",
    "        if abs(x-1) < 1:\n",
    "            return 0.1\n",
    "        elif abs(x-1) < 0.1:\n",
    "            return 0.25\n",
    "        else:\n",
    "            return 0.01\n",
    "\n",
    "    return np.apply_along_axis(lambda x: [int(x < 1) if random.random() < addNoise(x) else int(x > 1)], 1, x_in)\n",
    "    \n",
    "y = true_y(x, N).flatten()\n",
    "\n",
    "plt.scatter(x[y == 1,0], y[y == 1], c='blue', s=100, alpha=0.2)\n",
    "plt.scatter(x[y == 0,0], y[y == 0], c='red', s=100, alpha=0.2)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend(('positive cases', 'negative cases'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly peek at the x and y data objects (i.e. numpy arrays) to see what their size, shape, and rank look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"y.size: {y.size}\")\n",
    "print(f\"y.shape: {y.shape}\")\n",
    "print(f\"y.ndim: {y.ndim}\")\n",
    "\n",
    "print(f\"x.size: {x.size}\")\n",
    "print(f\"x.shape: {x.shape}\")\n",
    "print(f\"x.ndim: {x.ndim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Fit Logistic Regression Model\n",
    "\n",
    "Let's find estimates for $\\beta_0$ and $\\beta_1$ using a logistic regression model in order to predict/estimate the probability that y is in the positive or negative class for any given x. In traditional statistics there is a large class of models known as [generalized linear models](https://en.wikipedia.org/wiki/Generalized_linear_model), which allow for a linear regression to be used to predict some transformation of the target variable. With logistic regression we transform the target variable, $Y$ (a binary 0 or 1) by looking at the log-odds of the expected value of $y_i$. If we let $\\mu_i = E(y_i)$ be the probability that $y_i$ equals 1, then we that the log-odds are as follows (note that the log-odds are formally known as the [logit](https://en.wikipedia.org/wiki/Logit) function):\n",
    "\n",
    "* $\\mathrm{logit}(\\mu_i) = ln\\Big( \\frac{\\mu_i}{1 - \\mu_i} \\Big)= \\beta_0 + \\beta_1*x_i$\n",
    "\n",
    "Notice that the right side of the formula is in the familiar linear regression format of $\\beta$ coefficients and their corresponding $x$ covariates. \n",
    "\n",
    "When it comes time to make a prediction for a new $y_i$, we'll then use the inverse of the logit function, which happens to be the logistic, or sigmoid, function:\n",
    "\n",
    "* $\\hat{\\mu}_i = \\mathrm{logit}^{-1}\\Big( \\mathrm{logit}(\\mu_i) \\Big) = \\mathrm{logit}^{-1}(\\beta_0 + \\beta_1*x_i) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1*x_i)}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg_model = LogisticRegression(random_state=42, max_iter=100, tol=1e-3, solver='liblinear')\n",
    "logreg_model.fit(x, y)\n",
    "\n",
    "print(f\" beta0 = {logreg_model.intercept_[0]:.4f}\")\n",
    "print(f\" beta1 = {logreg_model.coef_[0][0]:.4f}\")\n",
    "y_pred = logreg_model.predict_proba(x)\n",
    "lr_loss = 1/N * np.square(y - y_pred[:,1]).sum()\n",
    "print(f\" loss (mse) = {lr_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find those estimates scikit learn's LogisticRegression model solved an optimization problem that we are familiar with, which was to minimize the loss function. Recall that the mean-squared error loss function looks like this: \n",
    "\n",
    "* $\\mathrm{Loss}_{MSE} = \\frac{1}{N} \\sum_i^N (y_i - \\hat{y}_i)^2$\n",
    "\n",
    "\n",
    "If we state $\\hat{y}_i$ in terms of our model parameters then, as seen above, we have the following: \n",
    "\n",
    "* $ \\mathrm{Loss}_{MSE} = \\frac{1}{N} \\sum_i^N \\Big( y_i - (1 + e^{-(\\beta_0 + \\beta_1*x_i)})^{-1} \\Big)^2$\n",
    "\n",
    "By writing the loss function as a function of our model parameters, we can then plot the surface of the loss with $\\beta_0$ and $\\beta_1$ on the x and y axes, respectively. We can then see where the optimal values might be. Remember that in practice, when we have hundreds, thousands, or more, model parameters, that it is impossible to plot the loss surface in this way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b1s = np.arange(6, -4.1, -0.5)\n",
    "#b0s = np.arange(-6, 4.1, 0.5)\n",
    "b1s = np.arange(9, -5, -0.5)\n",
    "b0s = np.arange(-9, 5, 0.5)\n",
    "surf = np.array( [[1/N * np.square(y - 1 / (1 + np.exp(-1 * (b1s[i]*x[:,0] + b0s[j])))).sum() for j in range(len(b0s))] for i in range(len(b1s))] )\n",
    "df = pd.DataFrame(surf, columns=b0s, index=b1s)\n",
    "p1 = sns.heatmap(df, cbar_kws={'label': 'loss'}, cmap=\"RdYlGn_r\")\n",
    "plt.xlabel(\"beta0\")\n",
    "plt.ylabel(\"beta1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice above that there does not appear to be one unique pair of values for $\\beta_0$, $\\beta_1$ that will yield a minimum loss value. That is to say, there is no single point on the surface we plotted above where the loss has an obvivous global minimum. Instead, it looks as if the loss continues to decrease as $\\beta_0$ gets smaller and $\\beta_1$ gets larger. \n",
    "\n",
    "What would happen if we chose to use another loss function? Would the log-loss (aka binary cross-entropy) function look much different? The log-loss, or binary cross-entropy function is defined as follows. \n",
    "\n",
    "* $\\mathrm{Loss}_{BCE} = - \\sum_i^N \\Big( y_i * \\mathrm{ln}(\\hat{y}_i) + (1 - y_i)*\\mathrm{ln}(1-\\hat{y}_i) \\Big)$\n",
    "\n",
    "This is a different looking function altogether from the MSE. We would expect, however, that the surface would be similar though so that we would ultimately find similar parameter values. Let's plot this now to see. (Note that in practice the log-loss is the loss function typically used for logistic regression models.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b1s = np.arange(9, -4.1, -0.5)\n",
    "#b0s = np.arange(-9, 4.1, 0.5)\n",
    "b1s = np.arange(9, -5, -0.5)\n",
    "b0s = np.arange(-9, 5, 0.5)\n",
    "surf = np.array( [[ -((y * np.log2(1 / (1 + np.exp(-1 * (b1s[i]*x[:,0] + b0s[j])))) + (1-y) * np.log2(1 - 1 / (1 + np.exp(-1 * (b1s[i]*x[:,0] + b0s[j])))))/100).sum() for j in range(len(b0s))] for i in range(len(b1s))] )\n",
    "df = pd.DataFrame(surf, columns=b0s, index=b1s)\n",
    "p1 = sns.heatmap(df, cbar_kws={'label': 'loss'}, cmap=\"RdYlGn_r\")\n",
    "plt.xlabel(\"beta0\")\n",
    "plt.ylabel(\"beta1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the two surfaces are noticeably distinct from one another, they do appear to agree on what values of the parameters will minimize the loss. It appears that in both cases the loss will be minimized when $\\beta_0$ is a small value (far to the left on the graph) and $\\beta_1$ is a large value (up high on the graph)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: Predict y for a Newly Observed x\n",
    "\n",
    "Let's see now what the model looks like using the parameter estimates found above (i.e. $\\beta_0 = -2.17$, $\\beta_1 = 2.55$). Next let's assume we are given a new observation to make a prediction for, and for this new observation we have $x = 1.5$. We can then predict whether it is a positive or negative case by evaluating the logistic function at $x = 1.2$. This yields $\\hat{y} \\approx 0.7$ (see below). \n",
    "\n",
    "Since $0.5$ is the typical cutoff value for predicting a positive (vs negative label), and $0.7 > 0.5$, we would likely predict this to be a positive case. \n",
    "\n",
    "__TIP:__ Try changing the values of the model parameters $\\beta_0$ and $\\beta_1$ below. Using the plots of the loss surfaces above, try choosing values of the model parameters look like they will yield an even lower point on the surface (i.e. smaller value of loss function). What happens to the plot below when doing so? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b0 = -2.16\n",
    "b1 = 2.55\n",
    "\n",
    "plt.scatter(x[y == 1,0], y[y == 1], c='blue', alpha=0.4)\n",
    "plt.scatter(x[y == 0,0], y[y == 0], c='red', alpha=0.4)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "\n",
    "x_new = 1.2\n",
    "y_hat_new = 1 / (1 + np.exp(-b0 - b1*x_new))\n",
    "plt.scatter(x_new, y_hat_new, color=\"green\")\n",
    "plt.legend(('positive cases', 'negative cases', 'new prediction'), loc='upper left')\n",
    "\n",
    "xes = np.arange(-4, 4, 0.2)\n",
    "plt.plot(xes, 1/(1 + np.exp(-b0 - b1*xes)), 'k--')\n",
    "\n",
    "plt.show()\n",
    "y_hat_new\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. A Simple Perceptron\n",
    "\n",
    "\n",
    "### 1. Simple Perceptron w/ NumPy\n",
    "\n",
    "Using the same $x$ and $y$ data as before we will now see how a simple perceptron network with a sigmoid activiation function is equivalent to the logistic regression model above. Rather than $\\beta_0$, $\\beta_1$, the parameters to be estimated are generally referred to as the weight and bias terms ($w$ and $b$, respectively). For this simple case they differ only in name though, such that:\n",
    "* $\\beta_0 = b$\n",
    "* $\\beta_1 = w$\n",
    "\n",
    "To begin we will manually train the perceptron using numpy and gradient descent to minimize the loss. This will involve taking the derivatives of $w$ and $b$ (using the chain rule) to calculate the gradients in an interative process. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly initialize parameters to be estimated\n",
    "np.random.seed(42)\n",
    "w = np.random.randn(1)\n",
    "b = np.random.randn(1)\n",
    "\n",
    "# learning rate parameter\n",
    "learning_rate = 5e-1 # 0.5\n",
    "\n",
    "# keep track of loss to see how the optimization performs\n",
    "loss = []\n",
    "\n",
    "params = []\n",
    "\n",
    "# Begin gradient descent using all of the observations in each iteration\n",
    "for i in range(250):\n",
    "\n",
    "    params.append((w[0], b[0]))\n",
    "\n",
    "    # Forward pass: compute predicted y\n",
    "    lin_pred = w[0] * x[:,0] + b[0]\n",
    "    y_pred = 1 / (1 + np.exp(-1 * lin_pred)) \n",
    "\n",
    "    # Compute and store loss, and print occassionally\n",
    "    loss.append(1/N * np.square(y - y_pred).sum())\n",
    "    if i % 50 == 0:\n",
    "        print(f\"iteration {i}: loss = {loss[i]:.4f}, w = {w[0]:.4f}, b = {b[0]:.4f}\")\n",
    "\n",
    "    # Backprop to compute gradients of w and b with respect to log loss\n",
    "    dloss_dypred = -2.0 / N * (y - y_pred)\n",
    "    dypred_dlinpred = np.exp(-lin_pred) * (1 / np.square(1 + np.exp(-1 * lin_pred)))\n",
    "    dlinpred_dw = x[:,0]\n",
    "    dlinpred_db = 1\n",
    "\n",
    "    # Backprop to compute gradients of w and b with respect to loss and being careful not to sum up intermediate parts\n",
    "    #grad_w = -2 / N * (y - 1 / (1 + np.exp(-1 * lin_pred))) * (1 / np.square(1 + np.exp(-1 * lin_pred))) * x[:,0] * np.exp(-lin_pred) \n",
    "    #grad_b = -2 / N * (y - 1 / (1 + np.exp(-1 * lin_pred))) * (1 / np.square(1 + np.exp(-1 * lin_pred))) * np.exp(-lin_pred) \n",
    "\n",
    "    # Calculate gradients and update weight and bias parameters \n",
    "    grad_w = (dloss_dypred * dypred_dlinpred * dlinpred_dw).sum()\n",
    "    grad_b = (dloss_dypred * dypred_dlinpred * dlinpred_db).sum()\n",
    "    w -= learning_rate * grad_w # calculate w^(k+1) from w^(k)\n",
    "    b -= learning_rate * grad_b # calculate b^(k+1) from b^(k)\n",
    "\n",
    "print(f\" w = {w.item():.4f}\")\n",
    "print(f\" b = {b.item():.4f}\")\n",
    "\n",
    "#plt.plot(range(0,len(loss)), loss)\n",
    "#plt.title(\"training a perceptron w/ numpy\")\n",
    "#plt.xlabel(\"iteration of gradient descent\")\n",
    "#plt.ylabel(\"loss function value\")\n",
    "#plt.show()\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(18,4))\n",
    "ax1.plot(range(0,len(loss)), loss)\n",
    "ax1.set(xlabel=\"optimization iteration\", ylabel=\"loss\")\n",
    "ax2.plot(range(0,len(params)), [parm[0] for parm in params])\n",
    "ax2.set(xlabel=\"optimization iteration\", ylabel=\"parameter: w\")\n",
    "ax3.plot(range(0,len(params)), [parm[1] for parm in params])\n",
    "ax3.set(xlabel=\"optimization iteration\", ylabel=\"parameter: b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Simple Perceptron w/ PyTorch\n",
    "\n",
    "Now we will train a basic perceptron using PyTorch to take advantage of its ability to perform automatic differentation. This allows us to avoid calculating derivatives manually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Randomly initialize weights and other data\n",
    "torch.manual_seed(42)\n",
    "w = torch.randn(1, requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)\n",
    "x_tensor = torch.tensor(x)\n",
    "y_tensor = torch.tensor(y)\n",
    "learning_rate = 5e-1\n",
    "losses = []\n",
    "params = []\n",
    "\n",
    "# Carry out gradient descent \n",
    "for i in range(250):\n",
    "\n",
    "    params.append((w.item(), b.item()))\n",
    "\n",
    "    # Forward pass: compute predicted y\n",
    "    lin_pred = w * x_tensor + b\n",
    "    y_pred = lin_pred.flatten().sigmoid()\n",
    "\n",
    "    # Compute and store loss, and print occassionally \n",
    "    loss = 1/N * (y_tensor - y_pred).pow(2).sum()\n",
    "    losses.append(loss.item())\n",
    "    if i % 50 == 0:\n",
    "        print(f\"iteration {i}: loss = {loss.item():.4f}, w = {w[0]:.4f}, b = {b[0]:.4f}\")\n",
    "\n",
    "    # Backprop using PyTorch's automatic differentiation \n",
    "    loss.backward()\n",
    "    #losses.append(loss.item())\n",
    "\n",
    "    # Update parameters but don't include these calculations as part of underlying computational graph\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "        b -= learning_rate * b.grad\n",
    "\n",
    "    # Reset gradients for next iteration \n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "\n",
    "print(f\" w = {w.item():.4f}\")\n",
    "print(f\" b = {b.item():.4f}\")\n",
    "\n",
    "#plt.plot(range(0,len(losses)), losses)\n",
    "#plt.title(\"training a perceptron w/ pytorch\")\n",
    "#plt.xlabel(\"iteration of gradient descent\")\n",
    "#plt.ylabel(\"loss function value\")\n",
    "#plt.show()\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(18,4))\n",
    "ax1.plot(range(0,len(losses)), losses)\n",
    "ax1.set(xlabel=\"optimization iteration\", ylabel=\"loss\")\n",
    "ax2.plot(range(0,len(params)), [parm[0] for parm in params])\n",
    "ax2.set(xlabel=\"optimization iteration\", ylabel=\"parameter: w\")\n",
    "ax3.plot(range(0,len(params)), [parm[1] for parm in params])\n",
    "ax3.set(xlabel=\"optimization iteration\", ylabel=\"parameter: b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Using More PyTorch Capabilities\n",
    "\n",
    "### 1: Defining a Perceptron w/ PyTorch\n",
    "\n",
    "Now we will train a basic perceptron using PyTorch to define the model architecture itself, while continuing to take advantage of its automatic differentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly initialize weights and other data\n",
    "torch.manual_seed(42)\n",
    "#del w #= torch.randn(1, requires_grad=True).reshape(1,1)\n",
    "#del b #b = torch.randn(1, requires_grad=True).reshape(1,1)\n",
    "x_tensor = torch.tensor(x).float()\n",
    "y_tensor = torch.tensor(y).float().reshape(N, D_out)\n",
    "\n",
    "# Define and declare a pytorch perceptron using sigmoid activation function \n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1, 1), # linear layer\n",
    "    torch.nn.Sigmoid()     # activation function\n",
    ")\n",
    "\n",
    "# Define loss function to be used \n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "learning_rate = 5e-1\n",
    "losses = []\n",
    "params = []\n",
    "\n",
    "# Carry out gradient descent \n",
    "for i in range(250):\n",
    "\n",
    "    params.append(tuple([param.item() for param in model.parameters()]))\n",
    "\n",
    "    # Forward pass: compute predicted y\n",
    "    y_pred = model.forward(x_tensor)\n",
    "    \n",
    "    # Compute and store loss, and print occassionally \n",
    "    loss = loss_fn(y_pred, y_tensor)\n",
    "    losses.append(loss.item())\n",
    "    if i % 50 == 0:\n",
    "        print(f\"iteration {i}: loss = {loss.item():.4f}\")\n",
    "\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backprop using PyTorch's automatic differentiation \n",
    "    loss.backward()\n",
    "\n",
    "    # Update parameters but don't include these calculations as part of underlying computational graph\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param.data -= learning_rate * param.grad # w(k+1) = w(k) - learning_rate*gradient\n",
    "\n",
    "print(\"parameter values after training:\")\n",
    "for i, param in enumerate(model.parameters()):\n",
    "    print(f\"  param {i} est = {param.item():.4f}\")\n",
    "\n",
    "#plt.plot(range(0,len(losses)), losses)\n",
    "#p#lt.title(\"training a perceptron as a pytorch model\")\n",
    "#plt.xlabel(\"iteration of gradient descent\")\n",
    "#plt.ylabel(\"loss function value\")\n",
    "#plt.show()\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(18,4))\n",
    "ax1.plot(range(0,len(losses)), losses)\n",
    "ax1.set(xlabel=\"optimization iteration\", ylabel=\"loss\")\n",
    "ax2.plot(range(0,len(params)), [parm[0] for parm in params])\n",
    "ax2.set(xlabel=\"optimization iteration\", ylabel=\"parameter: w\")\n",
    "ax3.plot(range(0,len(params)), [parm[1] for parm in params])\n",
    "ax3.set(xlabel=\"optimization iteration\", ylabel=\"parameter: b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. PyTorch Optimizer\n",
    "\n",
    "Now we will train a basic perceptron using PyTorch to define the model itself and also to carry out the Adam optimization method (a extension of gradient descent that adaptively estimates gradients to be able to find minima more quickly). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly initialize weights and other data\n",
    "torch.manual_seed(42)\n",
    "x_tensor = torch.tensor(x).float()\n",
    "y_tensor = torch.tensor(y).float().reshape(N, D_out)\n",
    "\n",
    "# Declare a perceptron instance\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, D_out),\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "\n",
    "# Define loss function to be used \n",
    "loss_fn = torch.nn.MSELoss()\n",
    "#loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "learning_rate = 5e-1 # 0.05\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "losses = []\n",
    "params = []\n",
    "\n",
    "# Carry out gradient descent \n",
    "for i in range(200 + 1):\n",
    "\n",
    "    params.append(tuple([param.item() for param in model.parameters()]))\n",
    "\n",
    "    # Forward pass: compute predicted y\n",
    "    y_pred = model.forward(x_tensor)\n",
    "    \n",
    "    # Compute and store loss, and print occassionally \n",
    "    loss = loss_fn(y_pred, y_tensor)\n",
    "    losses.append(loss.item())\n",
    "    if i % 50 == 0:\n",
    "        print(f\"iteration {i}: loss = {loss.item():.4f}\")\n",
    "\n",
    "    # Zero all gradients before backward pass\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backprop then call optimizer step to update all (relevant) model parameters for us\n",
    "    loss.backward()\n",
    "    optimizer.step() # this calculates w(k+1) for us (for all parameters)\n",
    "\n",
    "print(\"w and b estimates:\")\n",
    "for param in model.parameters():\n",
    "    print(f\"    {param.item():.4f}\")\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(18,4))\n",
    "ax1.plot(range(0,len(losses)), losses)\n",
    "ax1.set(xlabel=\"optimization iteration\", ylabel=\"loss\")\n",
    "ax2.plot(range(0,len(params)), [parm[0] for parm in params])\n",
    "ax2.set(xlabel=\"optimization iteration\", ylabel=\"parameter: w\")\n",
    "ax3.plot(range(0,len(params)), [parm[1] for parm in params])\n",
    "ax3.set(xlabel=\"optimization iteration\", ylabel=\"parameter: b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TIP:__ Try changing the learning rate above to a value greater than 5e-2 (e.g. 5e-1) and then re-run the cell above. How many epochs are then needed to get the loss below 0.10? Is it possible to get the loss below 0.05? \n",
    "\n",
    "__TIP:__ Try changing the loss function from MSELoss() to BCELoss(). Does this affect the parameter estimates at all? Note that sometimes the choice of loss function can affect the parameter estimates but often times the choice of loss function will have a greater effect on how quickly the the optimal parameter values are found. When changing the loss function above, try to see if there are any differences in how the loss decreases, or how the parameter estimates change. Note that when much larger, complex, models are being trained, the choice of the loss function can make a larger difference than it would for this simple example we are looking at. \n",
    "\n",
    "### 3. PyTorch torch.nn.Module Base Class\n",
    "Finally, let's make use of the PyTorch base class for creating a neural network. The `torch.nn.Module` class is the base class that we will inherit from to create a neural network. Notice that to start we override the constructor, `__init__`, and that inside this method we define the various layers to our neural network. We can then define the `forward` method to behave specifically how we choose. In the example below we pass an additional flag to the `forward()` method to either calculate and return the final predicted probability (with `apply_sigmoid=True`) or to simply return the linear predictor (with `apply_sigmoid=False`). This can be useful for us in some instances. For example, the log-loss function (aka binary cross-entropy) can execute slightly more efficiently by taking advantage of the fact that the `log` and `exp` functions are inverses of one another (recall that the sigmoid function has `exp()` in it). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly initialize weights and other data\n",
    "torch.manual_seed(42)\n",
    "#w = torch.randn(1, requires_grad=True).reshape(1,1)\n",
    "#b = torch.randn(1, requires_grad=True).reshape(1,1)\n",
    "x_tensor = torch.tensor(x).float()\n",
    "y_tensor = torch.tensor(y).float().reshape(N, D_out)\n",
    "\n",
    "# Define our own ANN class \n",
    "class Perceptron(torch.nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Perceptron, self).__init__()\n",
    "        self.lay1 = torch.nn.Linear(input_dim, 1) # linear layer\n",
    "        self.act = torch.nn.Sigmoid()             # activation layer\n",
    "    def forward(self, x, apply_sigmoid=False):\n",
    "        output = self.lay1(x) \n",
    "        if apply_sigmoid:\n",
    "            output = self.act(output)\n",
    "        return output\n",
    "\n",
    "model = Perceptron(1)\n",
    "\n",
    "# Define loss function to be used \n",
    "loss_fn = torch.nn.MSELoss()\n",
    "#loss_fn = torch.nn.BCELoss()\n",
    "#loss_fn = torch.nn.BCEWithLogitsLoss() # expects only linear predictor (w/o sigmoid applied)\n",
    "learning_rate = 5e-1\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "losses = []\n",
    "params = []\n",
    "\n",
    "# Carry out gradient descent \n",
    "for i in range(200+1):\n",
    "\n",
    "    params.append(tuple([param.item() for param in model.parameters()]))\n",
    "\n",
    "    # Forward pass: compute predicted y\n",
    "    y_pred = model.forward(x_tensor, apply_sigmoid=True) #False)\n",
    "\n",
    "    # Compute and store loss, and print occassionally \n",
    "    loss = loss_fn(y_pred, y_tensor)\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    if i % 50 == 0:\n",
    "        print(f\"iteration {i}: loss = {loss.item():.4f}\") #\" w = {w[0]:.4f}, b = {b[0]:.4f}\")\n",
    "\n",
    "    # Zero all gradients before backward pass\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backpropagation: calculate loss for each model parameter and have optimizer update all of them\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"w and b estimates:\")\n",
    "for param in model.parameters():\n",
    "    print(f\"   {param.item():.4f}\")\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(18,4))\n",
    "ax1.plot(range(0,len(losses)), losses)\n",
    "ax1.set(xlabel=\"optimization iteration\", ylabel=\"loss\")\n",
    "ax2.plot(range(0,len(params)), [parm[0] for parm in params])\n",
    "ax2.set(xlabel=\"optimization iteration\", ylabel=\"parameter: w\")\n",
    "ax3.plot(range(0,len(params)), [parm[1] for parm in params])\n",
    "ax3.set(xlabel=\"optimization iteration\", ylabel=\"parameter: b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one remaining issue that we have not dealt with yet, and that you might be wondering about. If you recall, Deep Learning is not only about using large neural network models with many hidden layers. It's also often relies on large, voluminous sets of data, oftentimes so large that calculating the gradients becomes computationally impractical, and sometimes so large that it all can't even be fit into memory at one time. In the next notebook we'll see how this is dealt with, how this affects optimization (and the surface of the loss function), and how this can be advantageous as well. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "5824d1dc66c06824c7e8b8389d411ffe3c70d55c506abb641232a99ffdaca160"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
