{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 39AA - Notebook 3: Improving our Airline Tweet Model\n",
    "\n",
    "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/sgeinitz/CS39AA/blob/main/nb3_improving_our_twitter_model.ipynb)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sgeinitz/CS39AA/blob/main/nb3_improving_our_twitter_model.ipynb)\n",
    "\n",
    "We'll now revisit the Airline Tweet dataset and try to resolve some of the issues that identified at the end of our first notebook, [Notebook 1](https://github/sgeinitz/CS39AA/blob/main/nb1_text_data_and_pandas.ipynb).\n",
    "\n",
    "If you recall, in Notebook 1, we created a very simple ad hoc model that simply counted the most frequently occuring words in each of the three distinct classes (positive, negative, and neutral). Our '_model_' then would take a new tweet and check to see how many times these words appeared. If more positive words appeared, then we assigned that label, and so on for negative and neutral. \n",
    "\n",
    "At the end we listed three issues or areas for improvement. These were:\n",
    "1. __Better Tokenization:__ The tokenization is very crude right now. Simply splitting a tweet into tokens with the 'SPACE' character as a delimiter means that positive tokens like _\"awesome.\"_ and _\"awesome!\"_ are considered as two different tokens. Cases such as that one are relatively easy to solve since it just meaning removing some punctuation. However, different forms of a word, such as _\"delay\"_ and _\"delayed\"_ can cause some issues too. For our ad hoc modeling approach above, resolving these tokenization don't seem to affect the outcome too much right now. But, once we try to use a proper model (e.g. naive bayes, neural network, etc.), then these tokenization deficiences can affect model performance even more. For example, having all of the different forms of the verb _\"delay\"_ will mean that our model needs to have that many more parameters in it. \n",
    "2. __Vectorization__: Even with better tokenization, we still need to do more modify the data to be able to use other types of models. What we specifically need to do is convert the tokens into a numerical representation of some kind. When working text data, this process of converting text/tokens into a numerical representation will allow us to use many different types of models, including neural networks. \n",
    "3. __Modeling Process/Evaluation:__ The modeling process and assessment need to be improved. To start, our accuracy of {{mod_accuracy}} is not much better than if we simply label every tweet as negative (since 65% of all tweets are negative). So we need to be sure that we're comparing our model performance metrics to a suitable baseline. The larger issue, however, is that we don't have a validation and/or test set right now. We used all 10k observations to build our positive, negative, and neutral sets of words; then we check our accuracy on these exact same sets of tweets. To really understand how our model will work for a new tweet that we have never seen, and that is posted in the future, we need to remove a portion of the dataset from the 'training', then evaluate our model against this. \n",
    "\n",
    "### 1. Our ad hoc Model Improved\n",
    "Let's reexamine our ad hoc model with all of those issues/improvements addressed. As before, we'll open the data file that is hosted on github and quickly check it's shape and peek at the first few rows (after changing the default display width for a pandas dataframe column). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_URL = 'https://raw.githubusercontent.com/sgeinitz/CS39AA/main/data/trainA.csv'\n",
    "df = pd.read_csv(data_URL)\n",
    "print(f\"df.shape: {df.shape}\")\n",
    "pd.set_option(\"display.max_colwidth\", 240)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that about 2/3 of the data have negative labels, and that the remaining labels are roughly split between positive and neutral (slightly more neutral than positive). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sentiment.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the nltk TweetTokenizer, which will split the text into separate words and characters based on common Twitter conventions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tk = TweetTokenizer()\n",
    "df['tokens_raw'] = df['text'].apply(lambda x: tk.tokenize(x.lower()))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's remove common stop words (e.g. \"_the_\", \"_in_\", etc.). In this next cell we will also remove some characters/punctuation, as well as hashtag tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words('english'))\n",
    "chars2remove = set(['.','!','/', '?'])\n",
    "df['tokens_raw'] = df['tokens_raw'].apply(lambda x: [w for w in x if w not in stops])\n",
    "df['tokens_raw'] = df['tokens_raw'].apply(lambda x: [w for w in x if w not in chars2remove])\n",
    "df['tokens_raw'] = df['tokens_raw'].apply(lambda x: [w for w in x if not re.match('^#', w)]) # remove hashtags\n",
    "#df['tokens_raw'] = df['tokens_raw'].apply(lambda x: [w for w in x if not re.match('^http', w)]) # remove web links\n",
    "#df['tokens_raw'] = df['tokens_raw'].apply(lambda x: [w for w in x if not re.match('^@', w)]) # remove web links\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the final step of text pre-processing we will lemmatize the tokens. Note that there are much better ways to do this but that we want to use a simple lemmatizer. For example, some lemmatizers also utilize a model internally to predict the part-of-speech for each word, since whether the word is a noun, adjective, verb, etc. will affect how lemmatization is done. Since we want to keep things simple here, and focus only on the lemmatization step, we'll assume every word is the same part of speech. Note that this is not by any means ideal (try to identify the incorrectly lemmatized token in the five tweets printed out below). In practice we would certainly utilize a 'smarter' lemmatizer ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "# also need to run following one time on your system (can be done outside of this notebook)\n",
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['tokens'] = df['tokens_raw'].apply(lambda x: [lemmatizer.lemmatize(w, pos=\"v\") for w in x])\n",
    "#df['tokens'] = df['tokens_raw'].apply(lambda x: [lemmatizer.lemmatize(w) for w in x])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since each tweet is currently stored as a string we then created a new column that was a list of each of the words in the tweet (since the default delimiter is a space character). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before we do the prediction let's split the dataset into validation and training subsets\n",
    "# (note that we could use sklearn.model_selection.train_test_split() for this, but we'll do it manually here)\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "df_full = df.copy()\n",
    "assert df_full.shape[0] == 10000\n",
    "\n",
    "indices = list(range(df_full.shape[0]))\n",
    "\n",
    "random.shuffle(indices)\n",
    "\n",
    "train_indices = indices[:8500]\n",
    "test_indices = indices[8500:]\n",
    "\n",
    "# df will be the in-sample training dataset\n",
    "df = df_full.iloc[train_indices,:].copy()\n",
    "print(f\"df.shape: {df.shape}\")\n",
    "\n",
    "# df_test will be the out-of-sample validation dataset\n",
    "df_test = df_full.iloc[test_indices,:].copy()\n",
    "print(f\"df_test.shape: {df_test.shape}\")\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we created a vocabularly sorted by frequency for the full dataset, the subset of positive tweets, negative tweets, and neutral tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset the data by positive, negative, and neutral\n",
    "df_pos = df[df['sentiment'] == 'positive']\n",
    "df_neg = df[df['sentiment'] == 'negative']\n",
    "df_neu = df[df['sentiment'] == 'neutral']\n",
    "\n",
    "def create_vocab_list(tokens_column):\n",
    "    vocab = dict()\n",
    "    for tweet_tokens in tokens_column:\n",
    "        for token in tweet_tokens:\n",
    "            if token not in vocab:\n",
    "                vocab[token] = 1\n",
    "            else:\n",
    "                vocab[token] += 1\n",
    "    return vocab\n",
    "\n",
    "vocab_all = dict(sorted(create_vocab_list(df['tokens']).items(), key=lambda item: item[1], reverse=True))\n",
    "vocab_pos = dict(sorted(create_vocab_list(df_pos['tokens']).items(), key=lambda item: item[1], reverse=True))\n",
    "vocab_neg = dict(sorted(create_vocab_list(df_neg['tokens']).items(), key=lambda item: item[1], reverse=True))\n",
    "vocab_neu = dict(sorted(create_vocab_list(df_neu['tokens']).items(), key=lambda item: item[1], reverse=True))\n",
    "print(f\"number of unique tokens overall: {len(vocab_all)}, pos tokens: {len(vocab_pos)}, neg: {len(vocab_neg)}, neu: {len(vocab_neu)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that in our original notebook we had about ~20k unique tokens, and this is now down to ~10k. Even though this is an ad hoc model, it is likely still a good simplification of the data. If we were using a real model, then this will be really beneficial since it will directly translate to the number of parameters in our model (decreasing the number of parameters is usually good, particularly when we can maintain similar performance, and/or we have a small dataset). As we did last time, let's look at the most frequently occuring words across all classes (ignoring whether it's positive, negative, or neutral)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(vocab_all.items())[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we already removed stop words during our tokenization step, it still seems from the output above that many of the most common occuring words across the full dataset are not very informative. So we will likely still end up benefiting from removing some of these words from the positive, negative, and neutral vocabularies that we created. The value of $500$ that we chose seems arbtrary here but in practice we would assess the model performance (on the validation dataset) for many different values both less than and greater than $500$. Even though our model is ad hoc, we can this of this number as a [__hyperparameter__](https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)) since it is not directly learned from the data but can be used to tune the performance of the model. In practice, we would want to look at the accuracy for both the training and validation data across many different values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_to_remove = 1000 #500 #500 #1000 # for our ad hoc model we can think of this as a type of hyperparameter\n",
    "for i, item in enumerate(vocab_all.items()):\n",
    "    if i == top_n_to_remove:\n",
    "        break\n",
    "    #print(f\" removing token: {item[0]:15} (w/ freq = {item[1]:5}) from vocabs\")\n",
    "    if item[0] in vocab_pos:\n",
    "        del vocab_pos[item[0]]\n",
    "    if item[0] in vocab_neg:\n",
    "        del vocab_neg[item[0]]\n",
    "    if item[0] in vocab_neu:\n",
    "        del vocab_neu[item[0]]\n",
    "\n",
    "print(f\"number of unique tokens overall: {len(vocab_all)}, pos tokens: {len(vocab_pos)}, neg: {len(vocab_neg)}, neu: {len(vocab_neu)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(vocab_pos.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(vocab_neg.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(vocab_neu.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, those words look pretty good since they seem to have a positive, negative, neutral connotation, respectively.  Now, let's try classifying the tweets by looking at one and counting how many tokens it has from the top k tokens in the vocab_pos, vocab_neg, and vocab_neutral sets. Whichever vocab it has the greatest number of tokens from, let's classify it as that. \n",
    "\n",
    "To accomplish this let's first create a single object here that represents our 'model'. This object is a dictionary data type and holds the vocabulary for each of positive, negative, and neutral classes. Note that below we are including _all_ of the tokens for each label because we are simply using a '[:]' in the square brackets, but we could easily include just the top $p$ positive tokens, top $p$ negative, etc. for $p$ equal to, say, $100$ using '[:100]'. This can be considered another hyperparameter for ad hoc model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put positive, negative, neutral words together into a single object to create our 'model'\n",
    "classifier_tokens = {\"positive\": list(vocab_pos.keys())[:], \"negative\": list(vocab_neg.keys())[:], \"neutral\": list(vocab_neu.keys())[:]}\n",
    "\n",
    "# a sample tweet to classify\n",
    "tweet2classify_i = 45\n",
    "tweet2classify = df.iloc[tweet2classify_i,:]['tokens']\n",
    "print(f\"example of tweet to classify: \\n  sentiment = {df.iloc[tweet2classify_i,0]} \\n  tweet = {df.iloc[tweet2classify_i,1]}\")\n",
    "\n",
    "pos = 0\n",
    "neg = 0\n",
    "neu = 0\n",
    "for tok in tweet2classify:\n",
    "    if tok in classifier_tokens['positive']:\n",
    "        pos += 1\n",
    "    elif tok in classifier_tokens['negative']:\n",
    "        neg += 1\n",
    "    elif tok in classifier_tokens['neutral']:\n",
    "        neu += 1\n",
    "\n",
    "print(f\"\\n vocab counts: pos = {pos},  neg = {neg},  neu = {neu}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function to predict the sentiment of a tweet using our ad hoc approach of counting the occurrence of words from the positive, negative, and neutral vocabularies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tweet_sentiment(tweet_tokens):\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    neu = 0\n",
    "    for tok in tweet_tokens:\n",
    "        if tok in classifier_tokens['positive']:\n",
    "            pos += 1\n",
    "        elif tok in classifier_tokens['negative']:\n",
    "            neg += 1\n",
    "        elif tok in classifier_tokens['neutral']:\n",
    "            neu += 1\n",
    "    if pos > neg and pos > neu:\n",
    "        return \"positive\"\n",
    "    elif neu > pos and neu > neg:\n",
    "        return \"neutral\"\n",
    "    else:\n",
    "        return \"negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into train and validation/test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions for the training dataset and check the performance with accuracy and a confusion matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions for training dataset\n",
    "df['predicted_sentiment'] = df['tokens'].apply(lambda x: predict_tweet_sentiment(x))\n",
    "\n",
    "# check performance on training dataset\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix(df['sentiment'], df['predicted_sentiment']), display_labels=['negative', 'neutral', 'positive'])\n",
    "disp.plot()\n",
    "mod_accuracy = accuracy_score(df['sentiment'], df['predicted_sentiment'])\n",
    "print(f\"our ad hoc model's accuracy on training dataset: {mod_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now assess the performance of our model on the validation data (i.e. 'df_test'). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions for the validation/test dataset\n",
    "df_test['predicted_sentiment'] = df_test['tokens'].apply(lambda x: predict_tweet_sentiment(x))\n",
    "\n",
    "# check performance on validation/test dataset\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix(df_test['sentiment'], df_test['predicted_sentiment']), display_labels=['negative', 'neutral', 'positive'])\n",
    "disp.plot()\n",
    "mod_accuracy = accuracy_score(df_test['sentiment'], df_test['predicted_sentiment'])\n",
    "print(f\"our ad hoc model's accuracy on validation dataset: {mod_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's stop here and think about whether we are assessing the model correctly. That is:\n",
    "1. Are we using a training and validation datasets to get an idea of how our model's  __out-of-sample__ performance? \n",
    "2. Also, are we splitting the dataset at the right time to ensure there is no [__data leakage__](https://en.wikipedia.org/wiki/Leakage_(machine_learning))? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Using a \"real\" ML Model\n",
    "\n",
    "Now that we have tokenized the data, we can quickly vectorize it to get a numeric representation for each tweet. That will then give us the flexibility to select from a much broader range of commonly used machine learning models. We can still use the `df` data frame as the training dataset, and `df_test` as the validation dataset. \n",
    "\n",
    "Just to be sure, let's go ahead and drop the `predicted_sentiment` column from both data frames. We can also drop the `tokens_raw` column since we are no longer using it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'predicted_sentiment' in df.columns:\n",
    "    print(\"removing column, predicted_sentiment, from df\")\n",
    "    df.drop('predicted_sentiment', axis=1, inplace=True)\n",
    "\n",
    "if 'predicted_sentiment' in df_test.columns:\n",
    "    print(\"removing column, predicted_sentiment, from df_test\")\n",
    "    df_test.drop('predicted_sentiment', axis=1, inplace=True)\n",
    "\n",
    "print(f\"df.shape: {df.shape}\")\n",
    "print(f\"df_test.shape: {df_test.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input to the sklearn vectorizer function requires that each observation (i.e. tweet) is in the form of a string, rather than a list of tokens. So we first need to combine the individual tokens for each tweet back into a string, which we do here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['textclean'] = df['tokens'].apply(lambda x: ' '.join(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will load the term-frequency inverse-document-frequency vectorizer from sklearn, `TfidfVectorizer`, to convert each tweet into a vector. We'll go ahead and call the resulting vectorized data, `X`, or `X_train` since it is only the training dataset. As with conventional statistical models, \"_X_\" represents the set of predictors, or independent variables. \n",
    "\n",
    "Also, note that `TfidfVectorizer` is a powerful text processing object. It has the ability to remove stop words, strip symbols, and do much of the work that our manual tokenization did. As such, we could easily use the original tweet text here, but we'll go ahead and continue with our manually tokenized data in the column, `textclean`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#import seaborn as sns\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train = tfidf_vectorizer.fit_transform(df['textclean']).toarray()\n",
    "#X_train = tfidf_vectorizer.fit_transform(df['text']).toarray() # original tweet text (without our manual tokenization)\n",
    "\n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "#one_hot_vectorizer = CountVectorizer(binary=True)\n",
    "#tweet_matrix = one_hot_vectorizer.fit_transform(list(df['textclean']))#.toarray()\n",
    "\n",
    "print(f\"X_train.shape = {X_train.shape}\")\n",
    "type(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the data type for `X_train` is no longer a pandas data frame. Instead, it is a numpy ndarray. We will talk much more about the python module, numpy, and its data types in the coming week. For now, let's just think of it a more efficently implemented object than a pandas data frame (e.g. smaller memory footprint), and one that we can think of a more of a matrix than a table of data. This can be seen by how we can easily index into `X_train` without the need for methods such as `iloc` or `loc`. Here we will look at the first 5 rows and 10 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[:5, :10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the output above, and back at the dimensions of `X_train` two cells back, what does this tell you about the data (aside from the size of the training data)? Specifically, what do the number of columns represent?\n",
    "\n",
    "If you're not sure, then look at the output of the following cell. We are looking at the first row of the training dataset, `X_train` again, but looking only at the non-zero values. These are the term-frequency inverse-document-frequency values for this tweet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs1 = list(X_train[0,:])\n",
    "for i, tfidf_val in enumerate(obs1):\n",
    "    if tfidf_val > 0:\n",
    "        print(f\"obs1 token at column i={i}, has a non-zero TF-IDF value: {tfidf_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to, we could retrieve the token associated with each column of `X_train` by using the `tfidf_vectorizer`. Let's do that now just to see if we can try to recover the tweet. You'll notice that we can't recover the ordering of the words, but rather just the set of words that were in the tweet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs1 = list(X_train[0,:])\n",
    "for i, tfidf_val in enumerate(obs1):\n",
    "    if tfidf_val > 0:\n",
    "        print(f\"obs1 has a non-zero TF-IDF value: {tfidf_val} at col i={i} (associated with token: {tfidf_vectorizer.get_feature_names_out()[i]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify this by going back to our training dataset, `df`, and checking what the first observation was. Looking at the output below. As stated above, we see that the same words are present in the data but not necessarily in the same order. Disregarding word order when vectorizing text data is sometimes (informally) referred to as a \"[__bag-of-words__](https://en.wikipedia.org/wiki/Bag-of-words_model)\" approach. Often times, \"__bag-of-words__\" suggests that just the counts (i.e. __term frequency__) is used, rather than what we are using here, __term-frequency inverse-document-frequency__. Nonetheless, the underlying idea that words frequency is accounted for, but word order is neglected, is an important observation to make here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good default model to start with that does not require too much fine tuning, and generally performs well is a [__Random Forest__](https://en.wikipedia.org/wiki/Random_forest) model. We won't dig into the details of how an RF model works exactly, but so long as we know that it creates many decision trees on random subsets of the data, then you've got a decent idea. Before doing so, we also need to convert the labels (i.e. `df.sentiment`) to a numpy data type. We'll quickly do that now and look at the first few observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df.sentiment.to_numpy()\n",
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll not fit the random forest model on the training data and check the performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=200)\n",
    "model = model.fit(X_train, y_train)\n",
    "\n",
    "predictions_train = model.predict(X_train)\n",
    "predictions_train\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix(df['sentiment'], predictions_train), display_labels=['negative', 'neutral', 'positive'])\n",
    "disp.plot()\n",
    "print(f\"accuracy (on X_train): {accuracy_score(df['sentiment'], predictions_train):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! Accuracy of ~99% is much better than what we saw with our ad hoc model. Let's check and see how well the model does on the validation dataset now.\n",
    "\n",
    "Notice that we are using `tfidf_vectorizer.transform()` now, and not the same method we used before on the training dataset, which was `tfidf_vectorizer.fit_transform()`. The `fit_transform` method creates the vocabulary from the training data. The `transform` method will use the vocabulary that was previously made, but if it encounters a token in the validation data that was never seen in the training data, then it will simply ignore it. This is one reason why making sure that training data for models is regularly updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['textclean'] = df_test['tokens'].apply(lambda x: ' '.join(x))\n",
    "X_test = tfidf_vectorizer.transform(df_test['textclean']).toarray()\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test = model.predict(X_test)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix(df_test['sentiment'], predictions_test), display_labels=['negative', 'neutral', 'positive'])\n",
    "disp.plot()\n",
    "print(f\"accuracy (on X_test): {accuracy_score(df_test['sentiment'], predictions_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, the model performance on the validation data is not nearly as good as the performance on the training dataset. That's expected to some degree. \n",
    "\n",
    "We may be able to improve it slightly by finding the optimal value of one of the random forest hyperparameters, e.g. `n_estimators`. However, one of the strengths of random forest is not it does a good job of overfitting. Knowing that, how do you think smaller values of `n_estimators` will affect accuracy on the validation data? How do you think larger values will affect it? Try a few different values of `n_estimators`, both larger and smaller than the default of `n_estimators=100`, to see what happens.\n",
    "\n",
    "Below we go ahead and try many different hyperparameter values (for n_estimators) to see what the best value is. As can be seen, there is not an immediately obvious choice for the best value of the n_estimators hyperparameter, which is exactly why random forest is such a nice to model to use at the beginning. Admittedly, an RF model might not perform as well as some other machine learning models, but it is reliable and does a decent job at balancing the Bias-Variance Tradeoff right away without a lot of tuning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# A function to create and fit a RF with a specific number of trees\n",
    "def fitRFModel(hyperparam_value):\n",
    "    rf_model = RandomForestClassifier(n_estimators=hyperparam_value, random_state=5)\n",
    "    #rf_model = RandomForestClassifier(min_samples_split=hyperparam_value, random_state=1)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    y_train_pred_prob = rf_model.predict_proba(X_train)\n",
    "    y_train_pred = rf_model.predict(X_train)\n",
    "    y_val_pred_prob = rf_model.predict_proba(X_test)\n",
    "    y_val_pred = rf_model.predict(X_test)\n",
    "    train_loss = log_loss(df['sentiment'], y_train_pred_prob, labels=['negative', 'neutral', 'positive']) #, F.binary_cross_entropy(torch.tensor(y_train_pred), torch.tensor(y_train.to_numpy().astype(float)), reduction=\"mean\")\n",
    "    train_acc = accuracy_score(df['sentiment'], y_train_pred)\n",
    "    val_loss = log_loss(df_test['sentiment'], y_val_pred_prob, labels=['negative', 'neutral', 'positive']) #F.binary_cross_entropy(torch.tensor(y_val_pred), torch.tensor(y_val.to_numpy().astype(float)), reduction=\"mean\")\n",
    "    val_acc = accuracy_score(df_test['sentiment'], y_val_pred)\n",
    "    #return((train_loss.item(), val_loss.item()))\n",
    "    return (train_loss, val_loss, train_acc, val_acc)\n",
    "\n",
    "# Possible values of min_samples_split are 10 to 70 (by 5)\n",
    "hyp_param_vals = list(range(5,151,10))\n",
    "#hyp_param_vals = [5] + list(range(10,201,10))\n",
    "metrics = []\n",
    "for hp in hyp_param_vals:\n",
    "    metrics.append(fitRFModel(hp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "ax = fig.add_axes([0, 0, 1, 1]) #.1, 0.1, 0.8, 0.8]) # main axes\n",
    "ax.plot(hyp_param_vals, [metric[1] for metric in metrics], '--ro') # validattion loss\n",
    "ax.plot(hyp_param_vals, [metric[0] for metric in metrics], '--bo') # training loss\n",
    "ax.legend([\"Validation Loss\", \"Train Loss\"], loc=1)\n",
    "ax.set_xticks(hyp_param_vals)\n",
    "ax.set(xlabel=\"n_estimators\", ylabel=\"loss (lower is better)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 6))\n",
    "ax = fig.add_axes([0, 0, 1, 1]) #.1, 0.1, 0.8, 0.8]) # main axes\n",
    "ax.plot(hyp_param_vals, [metric[3] for metric in metrics], '--ro') # validattion accuracy\n",
    "ax.plot(hyp_param_vals, [metric[2] for metric in metrics], '--bo') # training accuracy\n",
    "ax.legend([\"Validation Accuracy\", \"Train Accuracy\"], loc=4)\n",
    "ax.set_xticks(hyp_param_vals)\n",
    "ax.set(xlabel=\"n_estimators\", ylabel=\"accuracy (higher is better)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "bb3398f4b21c7b026dd5874af3f954bf25f1e8ff81e25d82a94abcbbaacf760b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
