{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 39AA - Notebook 6: Minibatches and Dataloaders\n",
    "\n",
    "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/sgeinitz/CS39AA/blob/main/nb6_minibatches_and_dataloaders.ipynb)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sgeinitz/CS39AA/blob/main/nb6_minibatches_and_dataloaders.ipynb)\n",
    "\n",
    "In the previous notebook we trained a logistic regression model using Scikit Learn and then again implemented it as a neural network using PyTorch. \n",
    "\n",
    "For training the neural network we iterated over the entire dataset many times. In practice, this is not what is done. Instead, we will always use a subset of the training data to update the parameters estimates on (i.e. make predictions, calculate the loss, derive gradients, then update parameters).\n",
    "\n",
    "Let's first quickly get back to where we were last time by first generating the data of 100 observations, $i = 1, 2, \\dots$. Each observation, $i$, had a single __feature__, $x_i$ (aka predictor, independent variable, explanatory variable, etc.) and with an __outcome__ or __target__, $y_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "random.seed(1) \n",
    "np.random.seed(1)\n",
    "\n",
    "N = 100 # total number of observations\n",
    "D_in = 1 # input dimension (i.e. dimension of a single observation's x vector)\n",
    "D_out = 1 # output dimension (i.e. y), so just 1 for this example\n",
    "\n",
    "# Create random input data and derive the 'true' labels/output\n",
    "x = np.random.randn(N, D_in) + 1 \n",
    "def true_y(x_in, n_obs):\n",
    "    def addNoise(x):\n",
    "        if abs(x-1) < 1.0:\n",
    "            return 0.1\n",
    "        elif abs(x-1) < 0.1:\n",
    "            return 0.2\n",
    "        else:\n",
    "            return 0.025\n",
    "\n",
    "    return np.apply_along_axis(lambda x: [int(x < 1) if random.random() < addNoise(x) else int(x > 1)], 1, x_in)\n",
    "    \n",
    "y = true_y(x, N).flatten()\n",
    "\n",
    "plt.scatter(x[y == 1,0], y[y == 1], c='blue', alpha=0.4)\n",
    "plt.scatter(x[y == 0,0], y[y == 0], c='red', alpha=0.4)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend(('positive cases', 'negative cases'), loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then looked at the surface of the loss function as a function of the parameters, $b$, and $w$, to see what their ideal values should be. \n",
    "\n",
    "* $ \\mathrm{Loss}_{MSE} = \\frac{1}{N} \\sum_i^N \\Big( y_i - (1 + e^{-(\\beta_0 + \\beta_1*x_i)})^{-1} \\Big)^2$\n",
    "\n",
    "Since we want to minimize the loss function the parameter estimates will be where the surface is dark green in the plot shown below (i.e. small values of $b$ and large values of $w$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.arange(6, -4.1, -0.5)\n",
    "b = np.arange(-6, 4.1, 0.5)\n",
    "surf = np.array( [[1/N * np.square(y - 1 / (1 + np.exp(-1 * (w[i]*x[:,0] + b[j])))).sum() for j in range(len(b))] for i in range(len(w))] )\n",
    "df = pd.DataFrame(surf, columns=b, index=w)\n",
    "p1 = sns.heatmap(df, cbar_kws={'label': 'MSE loss'}, cmap=\"RdYlGn_r\")\n",
    "plt.xlabel(\"w\")\n",
    "plt.ylabel(\"b\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we trained the single-layer perceptron model (that is identical to logistic regression) using PyTorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "#del model \n",
    "\n",
    "# Randomly initialize weights and other data\n",
    "torch.manual_seed(42)\n",
    "x_tensor = torch.tensor(x).float()\n",
    "y_tensor = torch.tensor(y).float().reshape(N, D_out)\n",
    "\n",
    "# Define a Perceptron class \n",
    "class Perceptron(torch.nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Perceptron, self).__init__()\n",
    "        self.lay1 = torch.nn.Linear(input_dim, 1) # linear layer\n",
    "        self.act = torch.nn.Sigmoid()             # activation layer\n",
    "    def forward(self, x, apply_sigmoid=False):\n",
    "        output = self.lay1(x) \n",
    "        if apply_sigmoid:\n",
    "            output = self.act(output)\n",
    "        return output\n",
    "\n",
    "model = Perceptron(1)\n",
    "\n",
    "# Define loss function to be used \n",
    "#loss_fn = torch.nn.MSELoss()\n",
    "#loss_fn = torch.nn.BCELoss()\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss() # expects only linear predictor (w/o sigmoid applied)\n",
    "learning_rate = 5e-2\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "losses = []\n",
    "params = []\n",
    "\n",
    "# Carry out gradient descent \n",
    "for i in range(200+1):\n",
    "\n",
    "    params.append(tuple([param.item() for param in model.parameters()]))\n",
    "\n",
    "    # Forward pass: compute predicted y\n",
    "    y_pred = model.forward(x_tensor, apply_sigmoid=True)\n",
    "\n",
    "    # Compute and store loss, and print occassionally \n",
    "    loss = loss_fn(y_pred, y_tensor)\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    if i % 50 == 0:\n",
    "        print(f\"iteration {i}: loss = {loss.item():.4f}\") #\" w = {w[0]:.4f}, b = {b[0]:.4f}\")\n",
    "\n",
    "    # Zero all gradients before backward pass\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backprop then call optimizer step to update all (relevant) model parameters\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"w and b estimates:\")\n",
    "for param in model.parameters():\n",
    "    print(f\"   {param.item():.4f}\")\n",
    "\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(18,4))\n",
    "ax1.plot(range(0,len(losses)), losses)\n",
    "ax1.set(xlabel=\"training iteration\", ylabel=\"loss\")\n",
    "ax2.plot(range(0,len(params)), [parm[0] for parm in params])\n",
    "ax2.set(xlabel=\"training iteration\", ylabel=\"parameter: w\")\n",
    "ax3.plot(range(0,len(params)), [parm[1] for parm in params])\n",
    "ax3.set(xlabel=\"training iteration\", ylabel=\"parameter: b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Using Batches\n",
    "\n",
    "Now let's try training the same model but with batches (more precisely, mini-batches)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly initialize weights and other data\n",
    "torch.manual_seed(42)\n",
    "x_tensor = torch.tensor(x).float()\n",
    "y_tensor = torch.tensor(y).float().reshape(N, D_out)\n",
    "\n",
    "# Define a Perceptron class \n",
    "class Perceptron(torch.nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Perceptron, self).__init__()\n",
    "        self.lay1 = torch.nn.Linear(input_dim, 1)\n",
    "        self.act = torch.nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        output = self.lay1(x)\n",
    "        output = self.act(output)\n",
    "        return output\n",
    "\n",
    "# Declare a perceptron instance\n",
    "model = Perceptron(1)\n",
    "\n",
    "# Define loss function to be used \n",
    "n_epochs = 50\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "learning_rate = 5e-2\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "losses = []\n",
    "params = []\n",
    "\n",
    "batch_size = 25\n",
    "batch_iterations = int(N / batch_size)\n",
    "\n",
    "# Carry out gradient descent \n",
    "for i in range(1, n_epochs+1):\n",
    "\n",
    "    for j in range(batch_iterations):\n",
    "\n",
    "        params.append(tuple([param.item() for param in model.parameters()]))\n",
    "\n",
    "        x_t = x_tensor[j*batch_size:(j+1)*batch_size]\n",
    "        y_t = y_tensor[j*batch_size:(j+1)*batch_size]\n",
    "\n",
    "        # Forward pass: compute predicted y\n",
    "        y_pred = model.forward(x_t)\n",
    "    \n",
    "        # Compute and store loss, and print occassionally \n",
    "        loss = loss_fn(y_pred, y_t)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Zero all gradients before backward pass\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backprop then call optimizer step to update all (relevant) model parameters\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if i % 25 == 0:\n",
    "        print(f\"iteration {i}: loss = {loss.item():.4f}, w = {params[-1][0]:.4f}, b = {params[-1][1]:.4f}\")\n",
    "\n",
    "print(\"w and b estimates:\")\n",
    "for param in model.parameters():\n",
    "    print(f\"    {param.item():.4f}\")\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(18,4))\n",
    "ax1.plot(range(0,len(losses)), losses)\n",
    "ax1.set(xlabel=\"training iteration\", ylabel=\"loss\")\n",
    "ax2.plot(range(0,len(params)), [parm[0] for parm in params])\n",
    "ax2.set(xlabel=\"training iteration\", ylabel=\"parameter: w\")\n",
    "ax3.plot(range(0,len(params)), [parm[1] for parm in params])\n",
    "ax3.set(xlabel=\"training iteration\", ylabel=\"parameter: b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tensor[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we see that there is a systematic oscillation, or pattern, to the training. In practice we want to randomly select a mini-batch each time. We could implement such a sampling technique ourselves, but PyTorch has utilities for us already. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Batches w/ PyTorch Dataset and DataLoader\n",
    "\n",
    "To make it much easier to use batches, we'll first need to make use of the PyTorch Dataset class. The typical approach is to create a own child Dataset class that inherits from the PyTorch Dataset class. This way we can inherit the functionality we want while tailoring the class to behave exactly as we need it to (e.g. using names we want for x, y, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, x, y):\n",
    "        self.x = torch.tensor(x, dtype=torch.float64)\n",
    "        self.y = torch.tensor(y, dtype=torch.float64)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {'x': self.x[index], 'y': self.y[index]}\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "\n",
    "dataset = MyDataset(x, y)\n",
    "\n",
    "print(f\"Without dataset object x[4] is: {x[4:6]}\")\n",
    "\n",
    "print(f\"Using dataset object x[4] is: {dataset[4:6]}\")\n",
    "\n",
    "dataset[4:6]['x']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can use the PyTorch DataLoader class to make loading our data, and to generate a batch each time we want run a batch iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "\n",
    "#for batch_index, batch_data in enumerate(dataloader):\n",
    "#    print(\"batch\", batch_index, \":\", batch_data)\n",
    "for batch_data in dataloader:\n",
    "    print(\"batch: \", batch_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly initialize weights and other data\n",
    "torch.manual_seed(42)\n",
    "w = torch.randn(1, requires_grad=True).reshape(1,1)\n",
    "b = torch.randn(1, requires_grad=True).reshape(1,1)\n",
    "\n",
    "# Define MyDataset class and create an instance\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = torch.tensor(x, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32).reshape(N, D_in)\n",
    "    def __getitem__(self, index):\n",
    "        return {'x': self.x[index], 'y': self.y[index]}\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "dataset = MyDataset(x, y)\n",
    "\n",
    "\n",
    "# Define a Perceptron class \n",
    "class Perceptron(torch.nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Perceptron, self).__init__()\n",
    "        self.lay1 = torch.nn.Linear(input_dim, 1)\n",
    "        self.act = torch.nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        output = self.lay1(x)\n",
    "        output = self.act(output)\n",
    "        return output\n",
    "\n",
    "# Declare a perceptron instance\n",
    "model = Perceptron(1)\n",
    "\n",
    "# Define loss function to be used \n",
    "n_epochs = 100\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "learning_rate = 5e-1\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "losses = []\n",
    "params = []\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Carry out gradient descent \n",
    "for i in range(1, n_epochs+1):\n",
    "\n",
    "    for batch_index, batch_data in enumerate(dataloader):\n",
    "\n",
    "        params.append(tuple([param.item() for param in model.parameters()]))\n",
    "\n",
    "        # Forward pass: compute predicted y\n",
    "        y_pred = model.forward(batch_data['x'])\n",
    "    \n",
    "        # Compute and store loss, and print occassionally \n",
    "        loss = loss_fn(y_pred, batch_data['y'])\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Zero all gradients before backward pass\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backprop then call optimizer step to update all (relevant) model parameters\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if i % 25 == 0:\n",
    "        print(f\"iteration {i}: loss = {loss.item():.4f}, w = {params[-1][0]:.4f}, b = {params[-1][1]:.4f}\")\n",
    "\n",
    "\n",
    "print(\"w and b estimates:\")\n",
    "for param in model.parameters():\n",
    "    print(f\"    {param.item():.4f}\")\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(18,4))\n",
    "ax1.plot(range(0,len(losses)), losses)\n",
    "ax1.set(xlabel=\"optimization iteration\", ylabel=\"loss\")\n",
    "ax2.plot(range(0,len(params)), [parm[0] for parm in params])\n",
    "ax2.set(xlabel=\"optimization iteration\", ylabel=\"parameter: w\")\n",
    "ax3.plot(range(0,len(params)), [parm[1] for parm in params])\n",
    "ax3.set(xlabel=\"optimization iteration\", ylabel=\"parameter: b\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another tool we will use oftentimes from now on is from the torchsummary Python module, which will allow us to see all of the layers of a PyTorch model printed out. It will also output the number of parameters in each layer, and the dimensions of the data as it passes through each layer. This will be helpful in the future to understand our models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchsummary\n",
    "torchsummary.summary(model, (1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's take a quick peek at what the surface of the loss function looks like for different (mini)batch sizes. \n",
    "\n",
    "__TIP:__ Try changing the batch size parameter, `bs`, below to smaller and larger numbers to see how much it varies relative to the loss surface for all $n=100$ observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 10 # batch_size\n",
    "\n",
    "random.seed(1)\n",
    "inds = [random.randint(0,99) for r in range(bs*3)]\n",
    "\n",
    "x1 = np.array([x[inds[i],0] for i in range(bs)])\n",
    "y1 = np.array([y[inds[i]] for i in range(bs)])\n",
    "x2 = np.array([x[inds[i],0] for i in range(bs, bs*2)])\n",
    "y2 = np.array([y[inds[i]] for i in range(bs, bs*2)])\n",
    "x3 = np.array([x[inds[i],0] for i in range(bs*2, bs*3)])\n",
    "y3 = np.array([y[inds[i]] for i in range(bs*2, bs*3)])\n",
    "\n",
    "b1 = np.arange(6, -4.1, -0.5)\n",
    "b0 = np.arange(-6, 4.1, 0.5)\n",
    "\n",
    "surf1 = np.array( [[1/N * np.square(y1 - 1 / (1 + np.exp(-1 * (b1[i]*x1 + b0[j])))).sum() for j in range(len(b0))] for i in range(len(b1))] )\n",
    "df1 = pd.DataFrame(surf1, columns=b0, index=b1)\n",
    "\n",
    "surf2 = np.array( [[1/N * np.square(y2 - 1 / (1 + np.exp(-1 * (b1[i]*x2 + b0[j])))).sum() for j in range(len(b0))] for i in range(len(b1))] )\n",
    "df2 = pd.DataFrame(surf2, columns=b0, index=b1)\n",
    "\n",
    "surf3 = np.array( [[1/N * np.square(y3 - 1 / (1 + np.exp(-1 * (b1[i]*x3 + b0[j])))).sum() for j in range(len(b0))] for i in range(len(b1))] )\n",
    "df3 = pd.DataFrame(surf3, columns=b0, index=b1)\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(15,4))\n",
    "sns.heatmap(df1, ax=ax1, cmap=\"RdYlGn_r\")\n",
    "sns.heatmap(df2, ax=ax2, cmap=\"RdYlGn_r\")\n",
    "sns.heatmap(df3, ax=ax3, cbar_kws={'label': 'loss'}, cmap=\"RdYlGn_r\")\n",
    "plt.xlabel(\"beta0\")\n",
    "plt.ylabel(\"beta1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bb3398f4b21c7b026dd5874af3f954bf25f1e8ff81e25d82a94abcbbaacf760b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
